{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a9fe7-fd48-4885-b4d0-c77e8f683d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from random import randrange\n",
    "from time import time\n",
    "from tqdm import tqdm, tnrange\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.optimizer_v2.gradient_descent import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import Dense, MaxPooling2D, Flatten, Conv2D, Lambda, Dropout, LeakyReLU, BatchNormalization, Activation, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from mlxtend.evaluate import accuracy\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop,SGD, Nadam\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949618f-1f6e-40d6-8344-8c79718fa091",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # ignore tensorflow warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ac1db-7d61-4780-8aa6-a10fb33d9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"data/data_palm_vein/NIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae3cfe5-c7f6-4856-bd37-70b3a437b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path, xdim=128, ydim=128, nb_of_users = 500):\n",
    "    label_names = []\n",
    "    X = []\n",
    "    y = []\n",
    "    nb_of_users = nb_of_users - 1\n",
    "    count = 0\n",
    "    identity = -1\n",
    "    directories = os.listdir(path)\n",
    "    directories.sort()\n",
    "    for dirname in tqdm_notebook(directories, desc=\"Loading images...\"):\n",
    "        if dirname == \".DS_Store\": continue\n",
    "        label_names.append(dirname)\n",
    "        data_path = os.path.join(path + \"/\" + dirname, '*g')\n",
    "        files = glob.glob(data_path)\n",
    "        if identity >= nb_of_users: break\n",
    "        identity += 1\n",
    "        files.sort()\n",
    "        for f1 in files:\n",
    "            img = cv2.imread(f1, cv2.IMREAD_GRAYSCALE)\n",
    "            #img = cv2.imread(f1)\n",
    "            #img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
    "            img = cv2.resize(img,(int(xdim*1), int(ydim*1)))\n",
    "            #X.append([np.array(img), np.array(img), np.array(img)])\n",
    "            #X.append(np.array(img))\n",
    "            #X.append(np.array(img))\n",
    "            X.append(np.array(img))\n",
    "            #stacked_img = np.stack((img,)*3, axis=-1)\n",
    "            #X.append(stacked_img)\n",
    "            y.append(identity)\n",
    "            count += 1\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print(\"\\n ================= Summary of extraction ================= \\n\")\n",
    "    print(count, ' images lues')\n",
    "    print(\"\\nX.shape = \", X.shape)\n",
    "    print(\"y.shape = \", y.shape)\n",
    "    gc.collect()\n",
    "    return X, y, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad596e67-c4c3-4820-ae3a-4efb91978f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, label_names = load_img(path_data, nb_of_users=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf451e58-023f-4812-95b7-6121d2429b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 255.\n",
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "y = to_categorical(y)\n",
    "\n",
    "print(\"\\nPREPROCESSING DATA\")\n",
    "print(\"--------------------------------------------------------------------------------\\n\")\n",
    "print(\"\\nX shape : {:19}   |   y shape : {}\".format(str(X.shape), y.shape))\n",
    "print(\"\\n\\n--------------------------------------------------------------------------------\")\n",
    "print(\"Il y a {} utilisateur(s) et plus de {} images de veines palmaires dans le dataset prélevé.\".format(y.shape[1], y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976063c3-454a-404d-9a52-1b4a994365b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22, 24))\n",
    "print(\"\\n=============================== Show datasets =============================== \\n\\n\")\n",
    "\n",
    "for i in range(36):\n",
    "    ax = plt.subplot(6, 6, i + 1)\n",
    "    r = randrange(y.shape[0])\n",
    "    plt.imshow(X[r], cmap=plt.cm.binary)\n",
    "    title_obj = plt.title(\"Identity : {}\".format(np.argmax(y[r])))\n",
    "    plt.setp(title_obj, color='blue')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb2fec-65e2-433b-9116-252b08564aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "print(\"\\nSplitting data ...\\n\")\n",
    "print(\"\\n=============================== Splitting data =============================== \\n\\n\")\n",
    "print(\"X_train shape   : {:19}    |    y_train shape : {}\\n\".format(str(X_train.shape), y_train.shape))\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"X_val shape     : {:19}    |    y_val shape   : {}\\n\".format(str(X_val.shape), y_val.shape))\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"X_test shape    : {:19}    |    y_test shape  : {}\\n\".format(str(X_test.shape), y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393db96-1a87-4873-bd17-35b2e528e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create the model for Keras wrapper to scikit learn\n",
    "# we will optimize the type of pooling layer (max or average) and the activation function of the 2nd and 3rd convolution layers \n",
    "def create_model(pool_type='max', conv_activation='LeakyReLU', dropout_rate=0.15, classes=500):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # first layer: convolution\n",
    "    model.add(Conv2D(96, kernel_size=(7, 7), activation='LeakyReLU', input_shape=(128, 128, 1))) \n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size=(5, 5), activation=conv_activation))  \n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "    \n",
    "    model.add(Conv2D(384, kernel_size=(3, 3), activation='LeakyReLU'))  \n",
    "\n",
    "    model.add(Dropout(rate=dropout_rate))     \n",
    "    model.add(Flatten())         \n",
    "    model.add(Dense(128, activation='LeakyReLU')) # 64\n",
    "    if dropout_rate != 0:\n",
    "        model.add(Dropout(rate=dropout_rate)) \n",
    "        \n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile( \n",
    "        optimizer='nadam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        )\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=2, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.7, \n",
    "                                            min_lr=0.00000000001)    \n",
    "    return model, learning_rate_reduction\n",
    "\n",
    "cnn, lr_reduction = create_model(classes=y.shape[1])\n",
    "\n",
    "cnn.compile(\n",
    "  optimizer='nadam',\n",
    "  loss='categorical_crossentropy',  \n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "n_epochs = 40 # 30 \n",
    "n_epochs_cv = 10 # 10  # reduce number of epochs for cross validation for performance reason\n",
    "\n",
    "n_cv = 3\n",
    "validation_ratio = 0.10\n",
    "#define callbacks\n",
    "#early_stop = EarlyStopping(monitor = 'val_accuracy', mode = 'max', patience=5, restore_best_weights=True)\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1a11f7-a383-4d56-bd74-9c69e45a2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the default CNN model\n",
    "history = cnn.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=n_epochs,  \n",
    "    validation_data=(X_val, y_val), \n",
    "    batch_size=64,\n",
    "    callbacks=lr_reduction\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5420ae4c-0d89-496f-8681-7048b48380c2",
   "metadata": {},
   "source": [
    "!mkdir -p saved_model\n",
    "# Saving the model for Future Inferences\n",
    "model_json = cnn.to_json()\n",
    "with open(\"saved_model/model_identification_500users_60/40.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "cnn.save('saved_model/model_identification_500users_60/40.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0d1f6-b7bc-44ce-a38f-37c0ea0494eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = cnn.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\n ================= Evaluation : 1 layer model ================= \\n\")\n",
    "print(\"  Results : \\n\")\n",
    "print(\"Loss  : %.2f\" % (val[0]))\n",
    "print(\"Score : %.2f%%\" % (val[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f88baa-0a23-4f2c-b189-cd95a042445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(train):\n",
    "    accuracy = train.history['accuracy']\n",
    "    val_accuracy = train.history['val_accuracy']\n",
    "    epochs = range(len(accuracy))\n",
    "    plt.plot(epochs, accuracy, 'b', label='Score apprentissage')\n",
    "    plt.plot(epochs, val_accuracy, 'r', label='Score validation')\n",
    "    plt.title('Scores')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "def plot_loss(train):\n",
    "    loss = train.history['loss']\n",
    "    val_loss = train.history['val_loss']\n",
    "    epochs = range(len(loss))\n",
    "    plt.plot(epochs, loss, 'b', label='Loss apprentissage')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Loss validation')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_scores(history)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c102bb08-8e1a-476b-a88b-387ff4942854",
   "metadata": {
    "tags": []
   },
   "source": [
    "# hyperparameter optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# optimize model \n",
    "start = time()\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "# define parameters and values for grid search \n",
    "param_grid = {\n",
    "    'pool_type': ['max', 'average'],\n",
    "    'conv_activation': ['sigmoid', 'tanh'],    \n",
    "    'epochs': [n_epochs_cv],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5644bf61-4ba1-4668-98cb-6210acddf381",
   "metadata": {},
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=n_cv)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f20c702-2ae8-4117-9825-c96a9604d7e6",
   "metadata": {},
   "source": [
    "# summarize results\n",
    "print('time for grid search = {:.0f} sec'.format(time()-start))\n",
    "display_cv_results(grid_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tftest",
   "language": "python",
   "name": "tftest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
